#!/usr/bin/env python3
"""
v1.1 20250321 Muhammet Nergizci, COMET - University of Leeds

========
Overview
========
This script applies absolute azimuth offsets (`daz`)—referenced in the ITRF2014 no-net-rotation frame—
to the residual cumulative displacement time-series (`cum`) of Burst Overlap Interferometry (BOI) results
generated by LiCSBAS. The aim is to convert the relative BOI time series into an absolute reference frame.

The corrected time-series (`cum_abs`) is written back into the original `cum.h5` file,
along with the corresponding referenced `daz` vector.

This script is intended to be used **after** the time-series inversion step 
(e.g., following `LiCSBAS13_sb_inv.py`) when per-epoch azimuth offsets (e.g., ICC+SD total shifts)
have already been estimated using `daz_lib_licsar`.

=====
Usage
=====
LiCSBAS131_boi_absolute.py -t TS_DIR [-i cum_file] [--model]

 -t    Path to the LiCSBAS time-series directory (e.g., TS_GEOCml10)     [REQUIRED]
 -i    Name of the cumulative HDF5 file to process (default: cum.h5)     [OPTIONAL]
 --model Use RANSAC model for daz values (default: False)                [OPTIONAL]
 --tide Apply tide correction (default: False)                          [OPTIONAL]
 --iono Apply iono correction (default: False)                          [OPTIONAL]
 --refresh Remove existing datasets before writing new ones (default: False) [OPTIONAL]

Example:
--------
python LiCSBAS131_boi_absolute.py -t TS_GEOCml10 [--model]

This will:
 - Load the `cum.h5` time-series file from the specified folder
 - Read the pre-estimated `daz` (azimuth offsets) for each epoch via `daz_lib_licsar`
 - Interpolate `daz` to match the time steps of the BOI time-series cube
 - Reference both `cum` and `daz` to the first acquisition epoch
 - Save the absolute cumulative displacement (`cum_abs`) and aligned `daz` vector into `cum.h5`

==============
Output Variables
==============
 - cum_abs: absolute cumulative displacement (NumPy array of shape [ntime, ny, nx])
 - daz: interpolated and referenced azimuth offset time series (NumPy array of shape [ntime])

=============
Dependencies
=============
 - h5py, numpy, pandas, xarray
 - LiCSBAS libraries: `loadall2cube`, `daz_lib_licsar`
"""

#%% Change log
'''
v1.0 20250321 Muhammet Nergizci, COMET University of Leeds
- Original implementation
'''

#%% Import 
import os
import sys
import getopt
import h5py as h5
import numpy as np
import pandas as pd
import datetime as dt
import time
#from LiCSBAS_out2nc import loadall2cube
import daz_lib_licsar as dl
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from sklearn.linear_model import RANSACRegressor, LinearRegression

class Usage(Exception):
    """Usage context manager"""
    def __init__(self, msg):
        self.msg = msg


#%% Main
def main(argv=None):
    if argv is None:
        argv = sys.argv
        
    start = time.time()    
    #%%Set defaults
    tsadir = ''
    cumfile = None
    model = False
    tide_apply = False
    iono_apply = False
    refresh = False
    #%% Read options
    try:
        opts, _ = getopt.getopt(argv[1:], "ht:i:", ["help", "model", "tide", "iono", "refresh"])
        for o, a in opts:
            if o in ("-h", "--help"):
                print(__doc__)
                return 0
            elif o == "-t":
                tsadir = a
            elif o == "-i":
                cumfile = a
            elif o == "--model":
                model = True
            elif o == "--tide":
                tide_apply = True
            elif o == "--iono":
                iono_apply = True
            elif o == "--refresh":
                refresh = True
                
        if not tsadir:
            raise Usage("TS directory not given. Use -t to specify.")
        if not os.path.isdir(tsadir):
            raise Usage(f"TS directory does not exist: {tsadir}")


    except Usage as err:
        print("\nERROR:", file=sys.stderr)
        print("  "+str(err.msg), file=sys.stderr)
        print("\nFor help, use -h or --help.\n", file=sys.stderr)
        return 2
    
    #%%Directory settings  ##I try to keep the LiCSBAS format structure (MN)
    framedir=os.getcwd()
    frame=os.path.basename(framedir)
    
    #%%Read data information
    if not cumfile:
        cumfile=os.path.join(tsadir,'cum.h5')
    else:
        if not os.path.exists(cumfile):
            print('Error reading specified input file, please fix')
            return 2
        
    #%%Read cum.h5, add daz values and remove tide and iono corrections
    print(f"Reading cum.h5 from: {cumfile}")
    # Load data
    with h5.File(cumfile, 'r') as f:
        cum = f['cum'][()]
        imdates = f['imdates'][()].astype(str)
        
        # check if they exist.
        tide_exists = 'tide' in f
        iono_exists = 'iono' in f
        
        tide = f['tide'][()] if tide_exists else np.zeros_like(cum)
        iono = f['iono'][()] if iono_exists else np.zeros_like(cum)
        

    # Reference all to first epoch
    cum = cum - cum[0]
    if tide_exists:
        tide = tide - tide[0]
    if iono_exists:
        iono = iono - iono[0]
    

    # Get daz correction (azimuth ionospheric delay)
    dazes = dl.get_daz_frame(frame)[['epoch', 'daz']]
    dazes['epoch'] = pd.to_datetime(dazes['epoch'])
    dazes['daz'] = dazes['daz'] * 14000  # Convert to mm (scale for azimuth geometry)
    
    # Interpolate daz to match imdates
    df_daz = pd.DataFrame({'epoch': pd.to_datetime(imdates)})
    df_daz = df_daz.merge(dazes, on='epoch', how='left').sort_values('epoch')
    #### checking the sizes is not fit.
    df_daz = df_daz.drop_duplicates(subset='epoch')  # Just in case
    if len(df_daz) != len(imdates):
        raise ValueError(f"Mismatch in time steps after merge: imdates={len(imdates)}, df_daz={len(df_daz)}")
    #####
    df_daz['daz'] = df_daz['daz'].interpolate(method='nearest', limit_direction='both')
    
    ##Modelling the daz values with RANSAC
    # Convert epoch to numeric time in days
    t0 = df_daz['epoch'].min()
    df_daz['days'] = (df_daz['epoch'] - t0).dt.days

    X = df_daz['days'].values.reshape(-1, 1)
    y = df_daz['daz'].values
    mask = ~np.isnan(y)

    # Fit RANSAC
    if np.sum(mask) < 2:
        vel_ransac = np.nan
        intercept_ransac = np.nan
        df_daz['daz_model'] = np.nan
    else:
        reg = RANSACRegressor(base_estimator=LinearRegression()).fit(X[mask], y[mask])
        vel_ransac = reg.estimator_.coef_[0]
        intercept_ransac = reg.estimator_.intercept_
        df_daz['daz_model'] = reg.predict(X)  # model prediction for all dates]    
        # Replace DAZ values with model if the difference > threshold (250 mm)
        threshold = 200
        # adjustment = 350
        diff = np.abs(df_daz['daz'] - df_daz['daz_model'])
        df_daz['daz_mixed'] = np.where(diff > threshold, df_daz['daz_model'], df_daz['daz'])
        # df_daz['daz_mixed'] = df_daz['daz']  # initialize with original values
    
        # # Apply custom correction
        # mask_large_diff = diff > threshold
        # df_daz.loc[mask_large_diff & (df_daz['daz'] > df_daz['daz_model']), 'daz_mixed'] -= adjustment
        # df_daz.loc[mask_large_diff & (df_daz['daz'] < df_daz['daz_model']), 'daz_mixed'] += adjustment
        
        
        
        
    ##plotting daz and save
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.scatter(df_daz['epoch'], df_daz['daz'], color='red', alpha=0.6, s=20, label=frame)
    if 'daz_model' in df_daz:
        ax.plot(df_daz['epoch'], df_daz['daz_model'], '-', color='blue', lw=2, label='RANSAC Model')
        ax.scatter(df_daz['epoch'], df_daz['daz_mixed'], color='green', alpha=0.6, s=15, label='DAZ Mixed (Corrected)')

    # Set labels and grid
    ax.set_xlabel('Epoch')
    ax.set_ylabel('DAZ (mm)')
    ax.legend(fontsize=8)
    ax.grid(True)

    # Force last date into x-ticks
    last_date = df_daz['epoch'].max()
    xticks = list(ax.get_xticks())  # Default ticks
    xticks.append(mdates.date2num(last_date))  # Add last date in float format
    ax.set_xticks(sorted(set(xticks)))  # Ensure unique & sorted

    # Use date formatter
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    fig.autofmt_xdate()  # Auto-format angle of date labels

    # Save and show
    fig.tight_layout()
    plt.savefig(os.path.join(tsadir, f"{frame}_daz_plot.png"), dpi=150)
    
    ##Extract daz values from the dataframe  
    if model:
        print('RANSAC model supported is used for daz values')
        daz = df_daz['daz_mixed'].to_numpy()
        daz = daz - daz[0]  # Align to first epoch
    else:
        print('Original data is used for daz values')
        daz = df_daz['daz'].to_numpy()
        daz = daz - daz[0]  # Align to first epoch
    # Apply all corrections ##TODO save only final result when you satify with the results
    cum_abs = cum + daz[:, None, None] # Add daz correction
    
    # breakpoint()
    # apply tide and iono corrections if they exists
    if tide_apply:
        if tide_exists:
            cum_abs_notide = cum_abs - tide
        else:
            cum_abs_notide = cum_abs.copy()
    else:
        cum_abs_notide = cum_abs.copy()

    if iono_apply:
        if iono_exists:
            cum_abs_notide_noiono = cum_abs_notide - iono   #if the no set is hthe applied notide_noiono represents the noiono
        else:
            cum_abs_notide_noiono = cum_abs_notide.copy()
    else:
        cum_abs_notide_noiono = cum_abs_notide.copy()
        
    # Save corrected datasets based on availability and flags
    print(f"Writing corrected cumulative datasets to {cumfile} ...")
    with h5.File(cumfile, 'r+') as f:
        ##let's remove the previous datasets if they exist
        if refresh:
            for key in ['cum_abs', 'cum_abs_notide', 'cum_abs_noiono', 'cum_abs_notide_noiono', 'vel_abs', 'vel_abs_notide', 'vel_abs_noiono', 'vel_abs_notide_noiono']:
                if key in f:
                    del f[key]
                    
        # Always save raw cumulative
        if 'cum_abs' in f:
            del f['cum_abs']
        f.create_dataset('cum_abs', data=cum_abs.astype('float32'), compression='gzip')

        # Save tide-corrected if applied and data exists
        if tide_apply and tide_exists:
            if 'cum_abs_notide' in f:
                del f['cum_abs_notide']
            f.create_dataset('cum_abs_notide', data=cum_abs_notide.astype('float32'), compression='gzip')
        
        # Save iono-corrected if only iono corrections applied and exist
        if not (tide_apply and tide_exists) and iono_apply and iono_exists:
            if 'cum_abs_noiono' in f:
                del f['cum_abs_noiono']
            f.create_dataset('cum_abs_noiono', data=cum_abs_notide_noiono.astype('float32'), compression='gzip')
            
        # Save tide+iono-corrected if both corrections applied and exist
        if tide_apply and tide_exists and iono_apply and iono_exists:
            if 'cum_abs_notide_noiono' in f:
                del f['cum_abs_notide_noiono']
            f.create_dataset('cum_abs_notide_noiono', data=cum_abs_notide_noiono.astype('float32'), compression='gzip')
        

    #%% Finish
    elapsed_time = time.time()-start
    hour = int(elapsed_time/3600)
    minite = int(np.mod((elapsed_time/60),60))
    sec = int(np.mod(elapsed_time,60))
    print("\nElapsed time: {0:02}h {1:02}m {2:02}s".format(hour,minite,sec))

    print('\n{} Successfully finished!!\n'.format(os.path.basename(argv[0])))
    # print('Output directory: {}\n'.format(os.path.relpath(tsadir)))

#%% main
if __name__ == "__main__":
    sys.exit(main())
