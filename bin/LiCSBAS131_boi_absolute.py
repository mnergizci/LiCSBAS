#!/usr/bin/env python3
"""
v1.1 20250321 Muhammet Nergizci, COMET - University of Leeds

========
Overview
========
This script applies absolute azimuth offsets (`daz`)—referenced in the ITRF2014 no-net-rotation frame—
to the residual cumulative displacement time-series (`cum`) of Burst Overlap Interferometry (BOI) results
generated by LiCSBAS. The aim is to convert the relative BOI time series into an absolute reference frame.

The corrected time-series (`cum_abs`) is written back into the original `cum.h5` file,
along with the corresponding referenced `daz` vector.

This script is intended to be used **after** the time-series inversion step 
(e.g., following `LiCSBAS13_sb_inv.py`) when per-epoch azimuth offsets (e.g., ICC+SD total shifts)
have already been estimated using `daz_lib_licsar`.

=====
Usage
=====
apply_daz_to_cum.py -t TS_DIR [-i cum_file]

 -t    Path to the LiCSBAS time-series directory (e.g., TS_GEOCml10)     [REQUIRED]
 -i    Name of the cumulative HDF5 file to process (default: cum.h5)     [OPTIONAL]

Example:
--------
python apply_daz_to_cum.py -t /path/to/TS_GEOCml10

This will:
 - Load the `cum.h5` time-series file from the specified folder
 - Read the pre-estimated `daz` (azimuth offsets) for each epoch via `daz_lib_licsar`
 - Interpolate `daz` to match the time steps of the BOI time-series cube
 - Reference both `cum` and `daz` to the first acquisition epoch
 - Save the absolute cumulative displacement (`cum_abs`) and aligned `daz` vector into `cum.h5`

==============
Output Variables
==============
 - cum_abs: absolute cumulative displacement (NumPy array of shape [ntime, ny, nx])
 - daz: interpolated and referenced azimuth offset time series (NumPy array of shape [ntime])

=============
Dependencies
=============
 - h5py, numpy, pandas, xarray
 - LiCSBAS libraries: `loadall2cube`, `daz_lib_licsar`
"""

#%% Change log
'''
v1.0 20250321 Muhammet Nergizci, COMET University of Leeds
- Original implementation
'''

#%% Import 
import os
import sys
import getopt
import h5py as h5
import numpy as np
import pandas as pd
import datetime as dt
import time
#from LiCSBAS_out2nc import loadall2cube
import daz_lib_licsar as dl
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

class Usage(Exception):
    """Usage context manager"""
    def __init__(self, msg):
        self.msg = msg


#%% Main
def main(argv=None):
    if argv is None:
        argv = sys.argv
        
    start = time.time()    
    #%%Set defaults
    tsadir = ''
    cumfile = None
    
    #%% Read options
    try:
        opts, _ = getopt.getopt(argv[1:], "ht:i:", ["help"])
        for o, a in opts:
            if o in ("-h", "--help"):
                print(__doc__)
                return 0
            elif o == "-t":
                tsadir = a
            elif o == "-i":
                cumfile = a

        if not tsadir:
            raise Usage("TS directory not given. Use -t to specify.")
        if not os.path.isdir(tsadir):
            raise Usage(f"TS directory does not exist: {tsadir}")


    except Usage as err:
        print("\nERROR:", file=sys.stderr)
        print("  "+str(err.msg), file=sys.stderr)
        print("\nFor help, use -h or --help.\n", file=sys.stderr)
        return 2
    
    #%%Directory settings  ##I try to keep the LiCSBAS format structure (MN)
    framedir=os.getcwd()
    frame=os.path.basename(framedir)
    
    #%%Read data information
    if not cumfile:
        cumfile=os.path.join(tsadir,'cum.h5')
    else:
        if not os.path.exists(cumfile):
            print('Error reading specified input file, please fix')
            return 2
        
    #%%Read cum.h5, add daz values and remove tide and iono corrections
    print(f"Reading cum.h5 from: {cumfile}")
    # Load data
    with h5.File(cumfile, 'r') as f:
        cum = f['cum'][()]
        imdates = f['imdates'][()].astype(str)
        
        # check if they exist.
        tide_exists = 'tide' in f
        iono_exists = 'iono' in f
        
        tide = f['tide'][()] if tide_exists else np.zeros_like(cum)
        iono = f['iono'][()] if iono_exists else np.zeros_like(cum)
        

    # Reference all to first epoch
    cum = cum - cum[0]
    if tide_exists:
        tide = tide - tide[0]
    if iono_exists:
        iono = iono - iono[0]
    

    # Get daz correction (azimuth ionospheric delay)
    dazes = dl.get_daz_frame(frame)[['epoch', 'daz']]
    dazes['epoch'] = pd.to_datetime(dazes['epoch'])
    dazes['daz'] = dazes['daz'] * 14000  # Convert to mm (scale for azimuth geometry)
    ##plotting daz and save
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.scatter(dazes['epoch'], dazes['daz'], color='red', alpha=0.6, s=20, label=frame)

    # Set labels and grid
    ax.set_xlabel('Epoch')
    ax.set_ylabel('DAZ (mm)')
    ax.legend(fontsize=8)
    ax.grid(True)

    # Force last date into x-ticks
    last_date = dazes['epoch'].max()
    xticks = list(ax.get_xticks())  # Default ticks
    xticks.append(mdates.date2num(last_date))  # Add last date in float format
    ax.set_xticks(sorted(set(xticks)))  # Ensure unique & sorted

    # Use date formatter
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    fig.autofmt_xdate()  # Auto-format angle of date labels

    # Save and show
    fig.tight_layout()
    plt.savefig(os.path.join(tsadir, f"{frame}_daz_plot.png"), dpi=150)
    
    # Interpolate daz to match imdates
    df_daz = pd.DataFrame({'epoch': pd.to_datetime(imdates)})
    df_daz = df_daz.merge(dazes, on='epoch', how='left').sort_values('epoch')
    #### checking the sizes is not fit.
    df_daz = df_daz.drop_duplicates(subset='epoch')  # Just in case
    if len(df_daz) != len(imdates):
        raise ValueError(f"Mismatch in time steps after merge: imdates={len(imdates)}, df_daz={len(df_daz)}")
    #####
    
    df_daz['daz'] = df_daz['daz'].interpolate(method='nearest', limit_direction='both')
    daz = df_daz['daz'].to_numpy()
    daz = daz - daz[0]  # Align to first epoch

    # Apply all corrections ##TODO save only final result when you satify with the results
    cum_abs = cum + daz[:, None, None] # Add daz correction
    # apply tide and iono corrections if they exists
    
    if tide_exists:
        cum_abs_notide = cum_abs - tide
    else:
        cum_abs_notide = cum_abs.copy()

    if iono_exists:
        cum_abs_notide_noiono = cum_abs_notide - iono   #if the no set is hthe applied notide_noiono represents the noiono
    else:
        cum_abs_notide_noiono = cum_abs_notide.copy()
    
    # Save corrected datasets
    print(f"Writing corrected cumulative datasets to {cumfile} ...")
    with h5.File(cumfile, 'r+') as f:
        for name, data in {
            'cum_abs': cum_abs,
            'cum_abs_notide': cum_abs_notide,
            'cum_abs_notide_noiono': cum_abs_notide_noiono
        }.items():
            if name in f:
                del f[name]
            f.create_dataset(name, data=data.astype('float32'), compression='gzip')
        

    #%% Finish
    elapsed_time = time.time()-start
    hour = int(elapsed_time/3600)
    minite = int(np.mod((elapsed_time/60),60))
    sec = int(np.mod(elapsed_time,60))
    print("\nElapsed time: {0:02}h {1:02}m {2:02}s".format(hour,minite,sec))

    print('\n{} Successfully finished!!\n'.format(os.path.basename(argv[0])))
    # print('Output directory: {}\n'.format(os.path.relpath(tsadir)))

#%% main
if __name__ == "__main__":
    sys.exit(main())
