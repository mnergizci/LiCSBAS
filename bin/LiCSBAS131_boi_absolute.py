#!/usr/bin/env python3
"""
v1.1 20250321 Muhammet Nergizci, COMET - University of Leeds

========
Overview
========
This script applies absolute azimuth offsets (`daz`)—referenced in the ITRF2014 no-net-rotation frame—
to the residual cumulative displacement time-series (`cum`) of Burst Overlap Interferometry (BOI) results
generated by LiCSBAS. The aim is to convert the relative BOI time series into an absolute reference frame.

The corrected time-series (`cum_abs`) is written back into the original `cum.h5` file,
along with the corresponding referenced `daz` vector.

This script is intended to be used **after** the time-series inversion step 
(e.g., following `LiCSBAS13_sb_inv.py`) when per-epoch azimuth offsets (e.g., ICC+SD total shifts)
have already been estimated using `daz_lib_licsar`.

=====
Usage
=====
apply_daz_to_cum.py -t TS_DIR [-i cum_file]

 -t    Path to the LiCSBAS time-series directory (e.g., TS_GEOCml10)     [REQUIRED]
 -i    Name of the cumulative HDF5 file to process (default: cum.h5)     [OPTIONAL]

Example:
--------
python apply_daz_to_cum.py -t /path/to/TS_GEOCml10

This will:
 - Load the `cum.h5` time-series file from the specified folder
 - Read the pre-estimated `daz` (azimuth offsets) for each epoch via `daz_lib_licsar`
 - Interpolate `daz` to match the time steps of the BOI time-series cube
 - Reference both `cum` and `daz` to the first acquisition epoch
 - Save the absolute cumulative displacement (`cum_abs`) and aligned `daz` vector into `cum.h5`

==============
Output Variables
==============
 - cum_abs: absolute cumulative displacement (NumPy array of shape [ntime, ny, nx])
 - daz: interpolated and referenced azimuth offset time series (NumPy array of shape [ntime])

=============
Dependencies
=============
 - h5py, numpy, pandas, xarray
 - LiCSBAS libraries: `loadall2cube`, `daz_lib_licsar`
"""

#%% Change log
'''
v1.0 20250321 Muhammet Nergizci, COMET University of Leeds
- Original implementation
'''

#%% Import 
import os
import sys
import getopt
import h5py as h5
import numpy as np
import pandas as pd
import datetime as dt
from LiCSBAS_out2nc import loadall2cube
import daz_lib_licsar as dl

class Usage(Exception):
    """Usage context manager"""
    def __init__(self, msg):
        self.msg = msg


#%% Main
def main(argv=None):
    if argv is None:
        argv = sys.argv
        
        
    #%%Set defaults
    tsadir = ''
    cumfile = None
    
    #%% Read options
    try:
        opts, _ = getopt.getopt(argv[1:], "ht:i:", ["help"])
        for o, a in opts:
            if o in ("-h", "--help"):
                print(__doc__)
                return 0
            elif o == "-t":
                tsadir = a
            elif o == "-i":
                cumfile = a

        if not tsadir:
            raise Usage("TS directory not given. Use -t to specify.")
        if not os.path.isdir(tsadir):
            raise Usage(f"TS directory does not exist: {tsadir}")


    except Usage as err:
        print("\nERROR:", file=sys.stderr)
        print("  "+str(err.msg), file=sys.stderr)
        print("\nFor help, use -h or --help.\n", file=sys.stderr)
        return 2
    
    #%%Directory settings  ##I try to keep the LiCSBAS format structure (MN)
    framedir=os.getcwd()
    frame=os.path.basename(framedir)
    
    #%%Read data information
    if not cumfile:
        cumfile=os.path.join(tsadir,'cum.h5')
    else:
        if not os.path.exists(cumfile):
            print('Error reading specified input file, please fix')
            return 2
        
    #%%Load cumulative time-series cube
    cube = loadall2cube(cumfile)
    first_date = cube.time.isel(time=0).values.astype(str).split('T')[0]
    cube['cum'] = cube['cum'] - cube['cum'].sel(time=first_date)

    # Create DataFrame from cube time steps
    df_cube = pd.DataFrame({'epoch': pd.to_datetime(cube['time'].values)})

    # Load azimuth ionospheric delay (daz) from frame 
    dazes = dl.get_daz_frame(frame)[['epoch', 'daz']]
    dazes['epoch'] = pd.to_datetime(dazes['epoch'])
    dazes['daz'] = dazes['daz'] * 14000 ## convert mm

    # Merge and interpolate 
    df_merged = pd.merge(df_cube, dazes, on='epoch', how='left')       # Keep all epochs from cube
    df_merged = df_merged.sort_values(by='epoch')                      # Ensure order
    df_merged['daz'] = df_merged['daz'].interpolate(method='nearest', limit_direction='both')                      # Fill nan daz values

    # Convert merged DataFrame to xarray format
    df_merged_xr = df_merged.set_index("epoch").to_xarray()

    # Align the daz column with the cube dataset based on the 'time' dimension
    cube = cube.assign(
        daz=("time", df_merged_xr.daz.reindex(time=cube["time"], method="nearest").data)
    )
    # Display the updated cube dataset
    cube['daz'] = cube['daz'] - cube['daz'].sel(time=first_date)
    cube['cum_absolute']=cube['cum']+cube['daz']
    
    #%% Save corrected datasets into cum.h5
    print(f"\nSaving 'cum_absolute' and 'daz' to {cumfile} ...")
    with h5.File(cumfile, 'r+') as hf:
        for key, data in {'cum_abs': cube['cum_absolute'].values, 'daz': cube['daz'].values}.items():
            if key in hf:
                del hf[key]
            hf.create_dataset(key, data=data)

    print("absolute BOI's saved successfully.")
#%% main
if __name__ == "__main__":
    sys.exit(main())
